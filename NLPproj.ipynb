{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f56b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ac345d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shibunice service fast easy everything also fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent service amar singh next time expect ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>excellent excellenti first time kebab curry re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according ambiance wanna give star restaurant ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazing place even better food geetika great h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  sentiment\n",
       "0  shibunice service fast easy everything also fo...          0\n",
       "1  excellent service amar singh next time expect ...          0\n",
       "2  excellent excellenti first time kebab curry re...          0\n",
       "3  according ambiance wanna give star restaurant ...          0\n",
       "4  amazing place even better food geetika great h...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\palash\\Downloads\\food_review_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2abbe372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7018,)\n",
      "X_valid: (2340,)\n",
      "y_train: (7018,)\n",
      "y_valid: (2340,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df[\"final_text\"],\n",
    "                                                      df[\"sentiment\"]  ,\n",
    "                                                      stratify = df[\"sentiment\"],\n",
    "                                                      test_size=0.25,\n",
    "                                                      random_state=42)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_valid: {X_valid.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_valid: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6764b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10726"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = []\n",
    "for review in X_train:\n",
    "    unique_words.extend(review.split())\n",
    "\n",
    "len(set(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78890e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ecdb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definitely', 'enjoable', 'place', 'jaipur', 'good']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ca015b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7018.000000\n",
       "mean       16.254773\n",
       "std        13.961594\n",
       "min         1.000000\n",
       "25%         6.000000\n",
       "50%        13.000000\n",
       "75%        20.000000\n",
       "max        81.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_review=[]\n",
    "for review in X_train:\n",
    "    len_review.append(len(review.split()))\n",
    "len_series = pd.Series(data= len_review)\n",
    "len_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973de8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(len_series)\n",
    "plt.axvline(len_series.mean(), color = \"red\",\n",
    "            label = f\"mean: {round(len_series.mean(),2)}\")\n",
    "plt.axvline(len_series.median(), color = \"green\",\n",
    "            label = f\"median: {round(len_series.median(),2)}\")\n",
    "plt.axvline(len_series.mode()[0], color = \"orange\",\n",
    "            label = f\"mode: {round(len_series.mode()[0],2)}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e313994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10726"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words in X_train\n",
    "vocab = []\n",
    "for review in X_train:\n",
    "  vocab.extend(review.split())\n",
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e60fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definitely',\n",
       " 'enjoable',\n",
       " 'place',\n",
       " 'jaipur',\n",
       " 'good',\n",
       " 'food',\n",
       " 'good',\n",
       " 'service',\n",
       " 'food',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'cocktail',\n",
       " 'crazy',\n",
       " 'af',\n",
       " 'come',\n",
       " 'place',\n",
       " 'day',\n",
       " 'love',\n",
       " 'food',\n",
       " 'excellent',\n",
       " 'service',\n",
       " 'excellent',\n",
       " 'foodthe',\n",
       " 'oder',\n",
       " 'chiken',\n",
       " 'birayani',\n",
       " 'paneer',\n",
       " 'tikka',\n",
       " 'masala',\n",
       " 'naan',\n",
       " 'roti',\n",
       " 'thank',\n",
       " 'kcco',\n",
       " 'nice',\n",
       " 'staff',\n",
       " 'great',\n",
       " 'food',\n",
       " 'cute',\n",
       " 'cosy',\n",
       " 'ambience',\n",
       " 'lot',\n",
       " 'experimental',\n",
       " 'option',\n",
       " 'amazing',\n",
       " 'taste',\n",
       " 'falafel',\n",
       " 'salad',\n",
       " 'pizza',\n",
       " 'awesome',\n",
       " 'place',\n",
       " 'amazing',\n",
       " 'wonderful',\n",
       " 'food',\n",
       " 'awesome',\n",
       " 'atmosphere',\n",
       " 'view',\n",
       " 'breath',\n",
       " 'taking',\n",
       " 'veg',\n",
       " 'maggi',\n",
       " 'one',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'go',\n",
       " 'amazing',\n",
       " 'view',\n",
       " 'best',\n",
       " 'place',\n",
       " 'visit',\n",
       " 'awesome',\n",
       " 'placei',\n",
       " 'ordered',\n",
       " 'gatta',\n",
       " 'masala',\n",
       " 'paneer',\n",
       " 'delicious',\n",
       " 'taking',\n",
       " 'also',\n",
       " 'helpful',\n",
       " 'good',\n",
       " 'photo',\n",
       " 'preparation',\n",
       " 'diff',\n",
       " 'order',\n",
       " 'excessive',\n",
       " 'salt',\n",
       " 'partially',\n",
       " 'cooked',\n",
       " 'bhoot',\n",
       " 'ghatiya',\n",
       " 'packing',\n",
       " 'hai',\n",
       " 'paisa',\n",
       " 'bhoot',\n",
       " 'jyada',\n",
       " 'amazing',\n",
       " 'experience',\n",
       " 'shivu',\n",
       " 'served',\n",
       " 'u',\n",
       " 'everything',\n",
       " 'polite',\n",
       " 'one',\n",
       " 'hell',\n",
       " 'dish',\n",
       " 'ambience',\n",
       " 'good',\n",
       " 'food',\n",
       " 'spicy',\n",
       " 'tasty',\n",
       " 'packing',\n",
       " 'leak',\n",
       " 'ordered',\n",
       " 'dish',\n",
       " 'executive',\n",
       " 'vegetarian',\n",
       " 'dimsums',\n",
       " 'asparagus',\n",
       " 'tempora',\n",
       " 'sushi',\n",
       " 'bestseller',\n",
       " 'luckily',\n",
       " 'clicked',\n",
       " 'photo',\n",
       " 'dimsums',\n",
       " 'eating',\n",
       " 'eaten',\n",
       " 'found',\n",
       " 'dead',\n",
       " 'cockroach',\n",
       " 'rim',\n",
       " 'dimsum',\n",
       " 'waiter',\n",
       " 'although',\n",
       " 'seem',\n",
       " 'realise',\n",
       " 'mistake',\n",
       " 'offered',\n",
       " 'u',\n",
       " 'another',\n",
       " 'dish',\n",
       " 'replacement',\n",
       " 'complementary',\n",
       " 'dessert',\n",
       " 'carelessness',\n",
       " 'could',\n",
       " 'lead',\n",
       " 'critical',\n",
       " 'health',\n",
       " 'issue',\n",
       " 'diner',\n",
       " 'attached',\n",
       " 'picture',\n",
       " 'reference',\n",
       " 'kindly',\n",
       " 'check',\n",
       " 'food',\n",
       " 'eating',\n",
       " 'ghee',\n",
       " 'nahi',\n",
       " 'daala',\n",
       " 'haddi',\n",
       " 'di',\n",
       " 'hai',\n",
       " 'pc',\n",
       " 'nahi',\n",
       " 'haibasi',\n",
       " 'ki',\n",
       " 'smell',\n",
       " 'aa',\n",
       " 'rahi',\n",
       " 'hsi',\n",
       " 'great',\n",
       " 'ambience',\n",
       " 'nice',\n",
       " 'foodeverything',\n",
       " 'superbeverything',\n",
       " 'top',\n",
       " 'bottomstart',\n",
       " 'service',\n",
       " 'endnice',\n",
       " 'place',\n",
       " 'hangout',\n",
       " 'family',\n",
       " 'friend',\n",
       " 'food',\n",
       " 'naan',\n",
       " 'pizza',\n",
       " 'katori',\n",
       " 'chaat',\n",
       " 'time',\n",
       " 'hitambience',\n",
       " 'drink',\n",
       " 'againjaipur',\n",
       " 'adda',\n",
       " 'name',\n",
       " 'suggests',\n",
       " 'adda',\n",
       " 'jaipurites',\n",
       " 'one',\n",
       " 'enjoy',\n",
       " 'fullest',\n",
       " 'no',\n",
       " 'competition',\n",
       " 'mean',\n",
       " 'many',\n",
       " 'club',\n",
       " 'lounge',\n",
       " 'energy',\n",
       " 'vibe',\n",
       " 'get',\n",
       " 'searching',\n",
       " 'place',\n",
       " 'enjoy',\n",
       " 'saturday',\n",
       " 'night',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'go',\n",
       " 'many',\n",
       " 'friend',\n",
       " 'family',\n",
       " 'peer',\n",
       " 'none',\n",
       " 'disappointed',\n",
       " 'courtesy',\n",
       " 'manager',\n",
       " 'team',\n",
       " 'polite',\n",
       " 'humble',\n",
       " 'love',\n",
       " 'serve',\n",
       " 'people',\n",
       " 'heart',\n",
       " 'reason',\n",
       " 'visit',\n",
       " 'want',\n",
       " 'party',\n",
       " 'celebrated',\n",
       " 'birthday',\n",
       " 'worst',\n",
       " 'place',\n",
       " 'thing',\n",
       " 'good',\n",
       " 'place',\n",
       " 'staff',\n",
       " 'service',\n",
       " 'food',\n",
       " 'quality',\n",
       " 'ambiance',\n",
       " 'place',\n",
       " 'rated',\n",
       " 'paneer',\n",
       " 'tasted',\n",
       " 'stale',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'good',\n",
       " 'service',\n",
       " 'surly',\n",
       " 'go',\n",
       " 'food',\n",
       " 'craving',\n",
       " 'loved',\n",
       " 'everything',\n",
       " 'perfect',\n",
       " 'place',\n",
       " 'quick',\n",
       " 'meal',\n",
       " 'good',\n",
       " 'food',\n",
       " 'great',\n",
       " 'vibe',\n",
       " 'prashant',\n",
       " 'waited',\n",
       " 'table',\n",
       " 'helpful',\n",
       " 'spent',\n",
       " 'lot',\n",
       " 'time',\n",
       " 'selecting',\n",
       " 'cake',\n",
       " 'special',\n",
       " 'occasion',\n",
       " 'delivered',\n",
       " 'different',\n",
       " 'cake',\n",
       " 'ordered',\n",
       " 'didnt',\n",
       " 'come',\n",
       " 'specially',\n",
       " 'taste',\n",
       " 'cake',\n",
       " 'pathetic',\n",
       " 'chinni',\n",
       " 'ke',\n",
       " 'rate',\n",
       " 'inhi',\n",
       " 'logo',\n",
       " 'ke',\n",
       " 'wajah',\n",
       " 'se',\n",
       " 'jayda',\n",
       " 'huee',\n",
       " 'hai',\n",
       " 'food',\n",
       " 'great',\n",
       " 'chicken',\n",
       " 'supremely',\n",
       " 'delicious',\n",
       " 'absolute',\n",
       " 'place',\n",
       " 'hangout',\n",
       " 'friend',\n",
       " 'taste',\n",
       " 'doesnt',\n",
       " 'justify',\n",
       " 'price',\n",
       " 'okay',\n",
       " 'become',\n",
       " 'one',\n",
       " 'favourite',\n",
       " 'place',\n",
       " 'visit',\n",
       " 'jaipur',\n",
       " 'not',\n",
       " 'food',\n",
       " 'not',\n",
       " 'speciality',\n",
       " 'padao',\n",
       " 'location',\n",
       " 'view',\n",
       " 'provides',\n",
       " 'guess',\n",
       " 'im',\n",
       " 'not',\n",
       " 'going',\n",
       " 'turn',\n",
       " 'towards',\n",
       " 'nahargarh',\n",
       " 'amber',\n",
       " 'ill',\n",
       " 'run',\n",
       " 'straight',\n",
       " 'padao',\n",
       " 'restaurant',\n",
       " 'sit',\n",
       " 'like',\n",
       " 'hour',\n",
       " 'enjoying',\n",
       " 'scenic',\n",
       " 'beauty',\n",
       " 'special',\n",
       " 'service',\n",
       " 'santosh',\n",
       " 'ji',\n",
       " 'feeling',\n",
       " 'like',\n",
       " 'top',\n",
       " 'world',\n",
       " 'best',\n",
       " 'time',\n",
       " 'would',\n",
       " 'monsoon',\n",
       " 'drizzling',\n",
       " 'little',\n",
       " 'amazing',\n",
       " 'chai',\n",
       " 'pakode',\n",
       " 'hand',\n",
       " 'oh',\n",
       " 'good',\n",
       " 'life',\n",
       " 'thing',\n",
       " 'thatd',\n",
       " 'come',\n",
       " 'mind',\n",
       " 'weve',\n",
       " 'one',\n",
       " 'experience',\n",
       " 'good',\n",
       " 'food',\n",
       " 'great',\n",
       " 'location',\n",
       " 'located',\n",
       " 'make',\n",
       " 'easily',\n",
       " 'accessible',\n",
       " 'people',\n",
       " 'area',\n",
       " 'visit',\n",
       " 'north',\n",
       " 'indian',\n",
       " 'food',\n",
       " 'starter',\n",
       " 'tasty',\n",
       " 'no',\n",
       " 'particular',\n",
       " 'fault',\n",
       " 'flavourful',\n",
       " 'pls',\n",
       " 'proper',\n",
       " 'management',\n",
       " 'new',\n",
       " 'year',\n",
       " 'eve',\n",
       " 'people',\n",
       " 'entered',\n",
       " 'capacity',\n",
       " 'no',\n",
       " 'food',\n",
       " 'waiting',\n",
       " 'two',\n",
       " 'hour',\n",
       " 'well',\n",
       " 'served',\n",
       " 'anshul',\n",
       " 'verma',\n",
       " 'food',\n",
       " 'awesome',\n",
       " 'service',\n",
       " 'present',\n",
       " 'fabulous',\n",
       " 'like',\n",
       " 'way',\n",
       " 'served',\n",
       " 'good',\n",
       " 'sense',\n",
       " 'serving',\n",
       " 'thankyou',\n",
       " 'sandeep',\n",
       " 'amazing',\n",
       " 'service',\n",
       " 'food',\n",
       " 'no',\n",
       " 'doubt',\n",
       " 'tasty',\n",
       " 'experience',\n",
       " 'kcco',\n",
       " 'wtp',\n",
       " 'recommended',\n",
       " 'must',\n",
       " 'visit',\n",
       " 'place',\n",
       " 'wtp',\n",
       " 'foodcourt',\n",
       " 'payment',\n",
       " 'made',\n",
       " 'liquor',\n",
       " 'not',\n",
       " 'applicable',\n",
       " 'zomato',\n",
       " 'pro',\n",
       " 'service',\n",
       " 'delayambient',\n",
       " 'sound',\n",
       " 'speaker',\n",
       " 'qualityloudness',\n",
       " 'not',\n",
       " 'bearable',\n",
       " 'also',\n",
       " 'not',\n",
       " 'clear',\n",
       " 'price',\n",
       " 'much',\n",
       " 'excellent',\n",
       " 'food',\n",
       " 'handi',\n",
       " 'chicken',\n",
       " 'jeera',\n",
       " 'rice',\n",
       " 'matka',\n",
       " 'chiken',\n",
       " 'really',\n",
       " 'tasty',\n",
       " 'food',\n",
       " 'excellent',\n",
       " 'service',\n",
       " 'shibu',\n",
       " 'ok',\n",
       " 'nice',\n",
       " 'good',\n",
       " 'palette',\n",
       " 'amazing',\n",
       " 'staff',\n",
       " 'food',\n",
       " 'good',\n",
       " 'staff',\n",
       " 'service',\n",
       " 'mr',\n",
       " 'shivu',\n",
       " 'best',\n",
       " 'humble',\n",
       " 'kind',\n",
       " 'serving',\n",
       " 'also',\n",
       " 'best',\n",
       " 'come',\n",
       " 'soon',\n",
       " 'excellent',\n",
       " 'food',\n",
       " 'shishi',\n",
       " 'paneer',\n",
       " 'lababder',\n",
       " 'excellent',\n",
       " 'sar',\n",
       " 'ice',\n",
       " 'mr',\n",
       " 'nirmal',\n",
       " 'thanks',\n",
       " 'nirmal',\n",
       " 'thanks',\n",
       " 'mccoy',\n",
       " 'food',\n",
       " 'good',\n",
       " 'nice',\n",
       " 'experience',\n",
       " 'good',\n",
       " 'series',\n",
       " 'hemraj',\n",
       " 'visited',\n",
       " 'place',\n",
       " 'day',\n",
       " 'backi',\n",
       " 'loved',\n",
       " 'ambiemce',\n",
       " 'looking',\n",
       " 'good',\n",
       " 'rooftop',\n",
       " 'place',\n",
       " 'u',\n",
       " 'must',\n",
       " 'visit',\n",
       " 'one',\n",
       " 'great',\n",
       " 'place',\n",
       " 'njoy',\n",
       " 'quality',\n",
       " 'time',\n",
       " 'loved',\n",
       " 'one',\n",
       " 'tried',\n",
       " 'kitkat',\n",
       " 'shake',\n",
       " 'paneer',\n",
       " 'tikka',\n",
       " 'chur',\n",
       " 'naan',\n",
       " 'service',\n",
       " 'good',\n",
       " 'staff',\n",
       " 'polite',\n",
       " 'food',\n",
       " 'taste',\n",
       " 'amazing',\n",
       " 'specially',\n",
       " 'chicken',\n",
       " 'biryani',\n",
       " 'nice',\n",
       " 'buffet',\n",
       " 'spread',\n",
       " 'elegantly',\n",
       " 'done',\n",
       " 'dessert',\n",
       " 'one',\n",
       " 'finest',\n",
       " 'coffee',\n",
       " 'shop',\n",
       " 'jaipur',\n",
       " 'minus',\n",
       " 'point',\n",
       " 'amazing',\n",
       " 'guy',\n",
       " 'trained',\n",
       " 'well',\n",
       " 'handle',\n",
       " 'guestsi',\n",
       " 'would',\n",
       " 'like',\n",
       " 'give',\n",
       " 'service',\n",
       " 'food',\n",
       " 'quantity',\n",
       " 'le',\n",
       " 'compared',\n",
       " 'full',\n",
       " 'plate',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'great',\n",
       " 'ambience',\n",
       " 'super',\n",
       " 'helping',\n",
       " 'friendly',\n",
       " 'staff',\n",
       " 'absolutely',\n",
       " 'perfect',\n",
       " 'place',\n",
       " 'family',\n",
       " 'outinggathering',\n",
       " 'went',\n",
       " 'celebrate',\n",
       " 'sister',\n",
       " 'birthday',\n",
       " 'enjoyed',\n",
       " 'fullest',\n",
       " 'must',\n",
       " 'try',\n",
       " 'dahi',\n",
       " 'kabeabs',\n",
       " 'super',\n",
       " 'awesome',\n",
       " 'poor',\n",
       " 'service',\n",
       " 'must',\n",
       " 'say',\n",
       " 'not',\n",
       " 'food',\n",
       " 'not',\n",
       " 'good',\n",
       " 'strangely',\n",
       " 'handled',\n",
       " 'attendant',\n",
       " 'table',\n",
       " 'cleaned',\n",
       " 'asking',\n",
       " 'time',\n",
       " 'water',\n",
       " 'served',\n",
       " 'order',\n",
       " 'came',\n",
       " 'couldnt',\n",
       " 'place',\n",
       " 'order',\n",
       " 'min',\n",
       " 'waiting',\n",
       " 'around',\n",
       " 'handful',\n",
       " 'customer',\n",
       " 'food',\n",
       " 'not',\n",
       " 'going',\n",
       " 'talk',\n",
       " 'sabji',\n",
       " 'good',\n",
       " 'item',\n",
       " 'bad',\n",
       " 'cold',\n",
       " 'pink',\n",
       " 'sauce',\n",
       " 'pasta',\n",
       " 'good',\n",
       " 'not',\n",
       " 'good',\n",
       " 'city',\n",
       " 'height',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'taste',\n",
       " 'wierd',\n",
       " 'guess',\n",
       " 'not',\n",
       " 'freshly',\n",
       " 'made',\n",
       " 'one',\n",
       " 'best',\n",
       " 'experience',\n",
       " 'highly',\n",
       " 'recommended',\n",
       " 'great',\n",
       " 'service',\n",
       " 'trained',\n",
       " 'staff',\n",
       " 'ambience',\n",
       " 'good',\n",
       " 'dal',\n",
       " 'tooo',\n",
       " 'much',\n",
       " 'garlicky',\n",
       " 'extremely',\n",
       " 'garlic',\n",
       " 'oil',\n",
       " 'even',\n",
       " 'couldnt',\n",
       " 'eat',\n",
       " 'bati',\n",
       " 'also',\n",
       " 'heavy',\n",
       " 'fruit',\n",
       " 'custard',\n",
       " 'amazing',\n",
       " 'le',\n",
       " 'portion',\n",
       " 'gone',\n",
       " 'someone',\n",
       " 'recommendation',\n",
       " 'found',\n",
       " 'service',\n",
       " 'not',\n",
       " 'goodi',\n",
       " 'told',\n",
       " 'staff',\n",
       " 'many',\n",
       " 'time',\n",
       " 'serve',\n",
       " 'food',\n",
       " 'not',\n",
       " 'done',\n",
       " 'way',\n",
       " 'much',\n",
       " 'gravy',\n",
       " 'pasta',\n",
       " 'tasteless',\n",
       " 'pasta',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'food',\n",
       " 'drink',\n",
       " 'everything',\n",
       " 'point',\n",
       " 'aesthetic',\n",
       " 'service',\n",
       " 'well',\n",
       " 'curated',\n",
       " 'menu',\n",
       " 'taste',\n",
       " 'lived',\n",
       " 'expectation',\n",
       " 'visit',\n",
       " 'explore',\n",
       " 'ambience',\n",
       " 'food',\n",
       " 'variety',\n",
       " 'service',\n",
       " 'short',\n",
       " 'went',\n",
       " 'buffet',\n",
       " 'lunch',\n",
       " 'beautiful',\n",
       " 'slightly',\n",
       " 'disappointed',\n",
       " 'one',\n",
       " 'couple',\n",
       " 'food',\n",
       " 'better',\n",
       " 'okra',\n",
       " 'great',\n",
       " 'spread',\n",
       " 'salad',\n",
       " 'amazing',\n",
       " 'presentation',\n",
       " 'special',\n",
       " 'mention',\n",
       " 'salad',\n",
       " 'watermelon',\n",
       " 'chestnut',\n",
       " 'italian',\n",
       " 'dressing',\n",
       " 'soup',\n",
       " 'tomato',\n",
       " 'basil',\n",
       " 'perfect',\n",
       " 'golgappe',\n",
       " 'pudina',\n",
       " 'water',\n",
       " 'good',\n",
       " 'option',\n",
       " 'making',\n",
       " 'bhel',\n",
       " 'according',\n",
       " 'come',\n",
       " 'main',\n",
       " 'course',\n",
       " 'thin',\n",
       " 'crust',\n",
       " 'italian',\n",
       " 'pizza',\n",
       " 'baked',\n",
       " 'wood',\n",
       " 'fire',\n",
       " 'oven',\n",
       " 'perfect',\n",
       " 'pasta',\n",
       " 'ambiance',\n",
       " 'perfect',\n",
       " 'blend',\n",
       " 'sophistication',\n",
       " 'comfort',\n",
       " 'creating',\n",
       " 'delightful',\n",
       " 'atmosphere',\n",
       " 'relaxing',\n",
       " 'meal',\n",
       " 'attention',\n",
       " 'detail',\n",
       " 'decor',\n",
       " 'lighting',\n",
       " 'table',\n",
       " 'setting',\n",
       " 'truly',\n",
       " 'stood',\n",
       " 'presentation',\n",
       " 'food',\n",
       " 'amazing',\n",
       " 'food',\n",
       " 'decent',\n",
       " 'enuf',\n",
       " 'tend',\n",
       " 'love',\n",
       " 'food',\n",
       " 'even',\n",
       " 'presented',\n",
       " 'welland',\n",
       " 'variety',\n",
       " 'drink',\n",
       " 'staff',\n",
       " 'good',\n",
       " 'ambience',\n",
       " 'place',\n",
       " 'small',\n",
       " 'elegant',\n",
       " 'interior',\n",
       " 'food',\n",
       " 'absolutely',\n",
       " 'horrible',\n",
       " 'presentation',\n",
       " 'horrible',\n",
       " 'service',\n",
       " 'horrible',\n",
       " 'ordered',\n",
       " 'grilled',\n",
       " 'vegetable',\n",
       " 'gave',\n",
       " 'portion',\n",
       " 'raw',\n",
       " 'eggplant',\n",
       " 'drink',\n",
       " 'essentially',\n",
       " 'coloured',\n",
       " 'water',\n",
       " 'soda',\n",
       " 'tempura',\n",
       " 'pathetic',\n",
       " 'licensed',\n",
       " 'chef',\n",
       " 'regret',\n",
       " 'bringing',\n",
       " 'guest',\n",
       " 'manager',\n",
       " 'tried',\n",
       " 'get',\n",
       " 'accountability',\n",
       " 'asking',\n",
       " 'name',\n",
       " 'unprofessional',\n",
       " 'unaccountable',\n",
       " 'disgusting',\n",
       " 'better',\n",
       " 'try',\n",
       " 'not',\n",
       " 'scam',\n",
       " 'customer',\n",
       " 'pitiful',\n",
       " 'food',\n",
       " 'service',\n",
       " 'good',\n",
       " 'food',\n",
       " 'excellent',\n",
       " 'mr',\n",
       " 'amar',\n",
       " 'singh',\n",
       " 'good',\n",
       " 'job',\n",
       " 'ordered',\n",
       " 'sour',\n",
       " 'dough',\n",
       " 'received',\n",
       " 'bread',\n",
       " 'cheating',\n",
       " 'first',\n",
       " 'time',\n",
       " 'giving',\n",
       " 'star',\n",
       " 'rating',\n",
       " 'food',\n",
       " 'ambience',\n",
       " 'service',\n",
       " 'food',\n",
       " 'perfect',\n",
       " 'start',\n",
       " 'end',\n",
       " 'presentation',\n",
       " 'food',\n",
       " 'add',\n",
       " 'taste',\n",
       " 'wont',\n",
       " 'find',\n",
       " 'tempting',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'anywhere',\n",
       " 'else',\n",
       " 'jaipur',\n",
       " 'good',\n",
       " 'service',\n",
       " 'ambience',\n",
       " 'head',\n",
       " 'mr',\n",
       " 'amar',\n",
       " 'singh',\n",
       " 'great',\n",
       " 'service',\n",
       " 'humble',\n",
       " 'great',\n",
       " 'staff',\n",
       " 'ordered',\n",
       " 'paneer',\n",
       " 'tikka',\n",
       " 'masala',\n",
       " 'main',\n",
       " 'course',\n",
       " 'chilli',\n",
       " 'paneer',\n",
       " 'starter',\n",
       " 'food',\n",
       " 'taste',\n",
       " 'quantity',\n",
       " 'really',\n",
       " 'good',\n",
       " 'service',\n",
       " 'really',\n",
       " 'quick',\n",
       " 'well',\n",
       " 'always',\n",
       " 'awesome',\n",
       " 'food',\n",
       " 'really',\n",
       " 'know',\n",
       " 'treat',\n",
       " 'guest',\n",
       " 'food',\n",
       " 'diehard',\n",
       " 'fan',\n",
       " 'whole',\n",
       " 'menui',\n",
       " 'went',\n",
       " 'malviya',\n",
       " 'nagar',\n",
       " 'outlet',\n",
       " 'rajapark',\n",
       " 'outlet',\n",
       " 'well',\n",
       " 'singapore',\n",
       " 'never',\n",
       " 'disappointed',\n",
       " 'thanks',\n",
       " 'sanjit',\n",
       " 'barmah',\n",
       " 'whole',\n",
       " 'team',\n",
       " 'kebab',\n",
       " 'special',\n",
       " 'request',\n",
       " 'please',\n",
       " 'come',\n",
       " 'udaipur',\n",
       " 'food',\n",
       " 'served',\n",
       " 'mr',\n",
       " 'shibu',\n",
       " 'super',\n",
       " 'delicious',\n",
       " 'service',\n",
       " 'even',\n",
       " 'betteri',\n",
       " 'loved',\n",
       " 'much',\n",
       " 'paneer',\n",
       " 'lababdar',\n",
       " 'amazing',\n",
       " 'test',\n",
       " 'always',\n",
       " 'fan',\n",
       " 'samrats',\n",
       " 'samosa',\n",
       " 'original',\n",
       " 'outlet',\n",
       " 'chaura',\n",
       " 'rasta',\n",
       " 'visited',\n",
       " 'masala',\n",
       " 'chowk',\n",
       " 'found',\n",
       " 'serve',\n",
       " 'variety',\n",
       " 'tried',\n",
       " 'dabeli',\n",
       " 'vada',\n",
       " 'pav',\n",
       " 'pyaaz',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cb8e6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.254773439726417"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training\n",
    "sum([len(review.split()) for review in X_train])/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aafebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28d692d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization  #https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=10726, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=17, # how long should the output sequence of tokens be?\n",
    "                                    pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e69ccfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcf6cc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.preprocessing.text_vectorization.TextVectorization at 0x246bdca2c80>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3ee2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best place visit awesome placei ordered gatta masala paneer delicious taking also helpful good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15, 4, 16, 36, 2170, 24, 1142, 121, 41, 29, 535, 13, 188, 3, 0, 0, 0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = X_train.iloc[4]\n",
    "print(sample_sentence)\n",
    "list(text_vectorizer([sample_sentence]).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69dab1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,    4,   16,   36, 2170,   24, 1142,  121,   41,   29,  535,\n",
       "         13,  188,    3,    0,    0,    0], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer([sample_sentence]).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ab7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss= text_vectorizer([\"Food is mainly composed of water, lipids, proteins, and carbohydrates.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0da884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 17), dtype=int64, numpy=\n",
       "array([[   2,    1, 1752,    1,    1,  228,    1,    1,    1,    1,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ada60359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10726\n",
      "Top 5 most common words: ['', '[UNK]', 'food', 'good', 'place']\n",
      "Bottom 5 least common words: ['aagya', 'aage', 'aagaya', 'aadiivaasii', 'aache']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acdc8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for review in X_train:\n",
    "  corpus.append(list(text_vectorizer([review]).numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3bf27b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 2, 3, 400, 142, 126, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus[:4]\n",
    "#len(corpus)\n",
    "list(text_vectorizer([review]).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dac25001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    vector_size = 128,\n",
    "    window=2,\n",
    "    min_count=0,\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acab0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edcbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "700ae42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7018"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a6187495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8380"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b13a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x246c10f88e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1262612d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1778519, 2982650)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(corpus, total_examples=model.corpus_count, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fac0d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10726"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f61be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c002879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8380"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89a3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9398344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aache'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()[10725]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1bb9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2346"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(range(0, 10726)) - set(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b056fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def gensim_to_keras_embedding(model,input_len = 17, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array\n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "        input_length = input_len\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be83d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = gensim_to_keras_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4fb94b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "overall perfect experience im happy frequent consumer      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 17, 128), dtype=float32, numpy=\n",
       "array([[[ 0.00469486,  0.00460428, -0.00203377, ..., -0.00388695,\n",
       "         -0.00688968, -0.00275155],\n",
       "        [ 0.00558798, -0.00689156,  0.00250641, ...,  0.00767071,\n",
       "         -0.00650883,  0.0047723 ],\n",
       "        [ 0.00110294,  0.00384858,  0.00402723, ..., -0.00461145,\n",
       "          0.00584636, -0.00567167],\n",
       "        ...,\n",
       "        [-0.00041893,  0.00018471,  0.00398699, ...,  0.00227689,\n",
       "         -0.00385377,  0.00343608],\n",
       "        [-0.00041893,  0.00018471,  0.00398699, ...,  0.00227689,\n",
       "         -0.00385377,  0.00343608],\n",
       "        [-0.00041893,  0.00018471,  0.00398699, ...,  0.00227689,\n",
       "         -0.00385377,  0.00343608]]], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "import random\n",
    "random_sentence = random.choice(X_train)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb1b0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input = layers.Input(shape = (1,), dtype = \"string\")\n",
    "tv = text_vectorizer(input)\n",
    "ebd = embedding(tv)\n",
    "ga = layers.GlobalAveragePooling1D()(ebd)\n",
    "# d1 = layers.Dense(64, activation = \"relu\")(ga)\n",
    "output = layers.Dense(1, activation = \"sigmoid\")(ga)\n",
    "\n",
    "model = tf.keras.Model(inputs = input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9378c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 17)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 17, 128)           1072640   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1072769 (4.09 MB)\n",
      "Trainable params: 129 (516.00 Byte)\n",
      "Non-trainable params: 1072640 (4.09 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19ac8364",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node model/embedding/embedding_lookup defined at (most recent call last):\n  File \"C:\\Users\\palash\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n\n  File \"C:\\Users\\palash\\AppData\\Local\\Temp\\ipykernel_10896\\2745785850.py\", line 9, in <module>\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[15,9] = 8979 is not in [0, 8380)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_311193]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# train_sentences, val_sentences, train_labels, val_labels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model/embedding/embedding_lookup defined at (most recent call last):\n  File \"C:\\Users\\palash\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n\n  File \"C:\\Users\\palash\\AppData\\Local\\Temp\\ipykernel_10896\\2745785850.py\", line 9, in <module>\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\palash\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[15,9] = 8979 is not in [0, 8380)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_311193]"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = \"binary_crossentropy\",\n",
    "    optimizer = \"adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "# train_sentences, val_sentences, train_labels, val_labels\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83188b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization  #https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=17) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True)\n",
    "text_vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "403b880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "ebd_layer = Embedding(\n",
    "    input_dim = len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim = 128,\n",
    "    input_length = 17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e33f6de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebd_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbf3ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 17)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 17, 128)           1373184   \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1373313 (5.24 MB)\n",
      "Trainable params: 1373313 (5.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = tf.keras.layers.Input(shape = (1,), dtype= \"string\")\n",
    "tv = text_vectorizer(input)\n",
    "ebd = ebd_layer(tv)\n",
    "ga = tf.keras.layers.GlobalAveragePooling1D()(ebd)\n",
    "# d1 = tf.keras.layers.Dense(16, activation = \"relu\")(ga)\n",
    "output = tf.keras.layers.Dense(1, activation = \"sigmoid\")(ga)\n",
    "\n",
    "model_ebd = tf.keras.Model(inputs = input, outputs = output)\n",
    "model_ebd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c87d6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.5194 - accuracy: 0.8058 - val_loss: 0.3551 - val_accuracy: 0.8949\n",
      "Epoch 2/4\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2740 - accuracy: 0.9248 - val_loss: 0.2488 - val_accuracy: 0.9124\n",
      "Epoch 3/4\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1970 - accuracy: 0.9410 - val_loss: 0.2212 - val_accuracy: 0.9201\n",
      "Epoch 4/4\n",
      "220/220 [==============================] - 6s 27ms/step - loss: 0.1600 - accuracy: 0.9521 - val_loss: 0.2155 - val_accuracy: 0.9265\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.layers import Embedding\n",
    "ebd_layer = Embedding(\n",
    "    input_dim = len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim = 128,\n",
    "    input_length = 17\n",
    ")\n",
    "input = tf.keras.layers.Input(shape = (1,), dtype= \"string\")\n",
    "tv = text_vectorizer(input)\n",
    "ebd = ebd_layer(tv)\n",
    "ga = tf.keras.layers.GlobalAveragePooling1D()(ebd)\n",
    "# d1 = tf.keras.layers.Dense(16, activation = \"relu\")(ga)\n",
    "output = tf.keras.layers.Dense(1, activation = \"sigmoid\")(ga)\n",
    "\n",
    "model_ebd = tf.keras.Model(inputs = input, outputs = output)\n",
    "\n",
    "model_ebd.compile(\n",
    "    loss = \"binary_crossentropy\",\n",
    "    optimizer = \"adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "result_ebd = model_ebd.fit(X_train, y_train, epochs = 4, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1294205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.73557264],\n",
       "       [0.9605301 ],\n",
       "       [0.95730984],\n",
       "       [0.16641568],\n",
       "       [0.96533275]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ebd = model_ebd.predict(X_valid)\n",
    "y_pred_ebd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b2b5187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_pred_ebd>=0.5, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4bc169af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ebd_flat = np.round(y_pred_ebd).flatten()\n",
    "y_pred_ebd_flat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe4b04a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6741    1\n",
       "5158    1\n",
       "102     1\n",
       "2167    1\n",
       "1130    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59ade9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      1410\n",
      "           1       0.91      0.90      0.91       930\n",
      "\n",
      "    accuracy                           0.93      2340\n",
      "   macro avg       0.92      0.92      0.92      2340\n",
      "weighted avg       0.93      0.93      0.93      2340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_valid, y_pred_ebd_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ab6c725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10728, 128)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebd_layer.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69098724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01325517, -0.02686268, -0.02637479, ...,  0.00028612,\n",
       "         0.00683883, -0.05066485],\n",
       "       [-0.04962243,  0.03594908, -0.02034652, ...,  0.01815004,\n",
       "         0.03257399, -0.04494863],\n",
       "       [-0.04467124,  0.07065123,  0.03170334, ...,  0.00147509,\n",
       "         0.01003034, -0.07288383],\n",
       "       ...,\n",
       "       [ 0.0547647 , -0.04700103, -0.02073087, ..., -0.00491996,\n",
       "         0.07418086,  0.07791632],\n",
       "       [-0.08009841,  0.07165869,  0.05388992, ...,  0.07147376,\n",
       "        -0.07955992, -0.03975553],\n",
       "       [-0.02444868,  0.10294838,  0.09063026, ...,  0.09739957,\n",
       "        -0.07856586, -0.0991958 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebd_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b515a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\palash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "# import emoji\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3845abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\palash\\Downloads\\sw_new.txt\", \"r\") as f:\n",
    "  sw_new = f.read()\n",
    "sw_new = sw_new.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb809117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text, sw = sw_new):\n",
    "  import nltk\n",
    "  import re\n",
    "  # import emoji\n",
    "  import string\n",
    "  from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "  # mobile_regex = \"(\\+*)((0[ -]*)*|((91 )*))((\\d{12})+|(\\d{10})+)|\\d{5}([- ]*)\\d{6}\"\n",
    "  url_regex = \"((http|https|www)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "  space_regex = \"\\s\\s+\"\n",
    "  # remove url\n",
    "  text = re.sub(url_regex, \"\", text)\n",
    "  # remove mobile\n",
    "  # text = re.sub(mobile_regex, \"\", text)\n",
    "  # lower casing\n",
    "  text = text.lower()\n",
    "  # remove emoji & punctuation & numbers\n",
    "  text = \"\".join([i for i in text if (ord(i) in range(97,123)) | (i == \" \")])\n",
    "  # remove multiple spaces\n",
    "  text = re.sub(space_regex, \" \", text)\n",
    "\n",
    "  # stopword removal\n",
    "  text = [i for i in text.split() if i not in sw]\n",
    "  # lemmatizing\n",
    "  lemma = WordNetLemmatizer()\n",
    "  text = \" \".join([lemma.lemmatize(i) for i in text])\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "846a0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"I went here alone. They don't having table for one person. Overall I think any of the other restaurants in the same block would be better choice. I hered many times, the name of chhabra's from Jaipur guys. Terrible.\"\n",
    "clean_text = text_cleaner(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d42ae50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went alone dont table one person overall think restaurant block would better choice hered many time name chhabras jaipur guy terrible'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "806f8473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.89597994]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ebd(tf.constant([clean_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0c04952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_ebd.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "735e8cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x21487c32e60>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "max_vocab_length = len(text_vectorizer.get_vocabulary())\n",
    "max_length = 14\n",
    "embedding = layers.Embedding(input_dim= max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize uniform\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\")\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a21d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input = layers.Input(shape = (1,), dtype = \"string\")\n",
    "tv = text_vectorizer(input)\n",
    "ebd = embedding(tv)\n",
    "ga = layers.GlobalAveragePooling1D()(ebd)\n",
    "# d1 = layers.Dense(64, activation = \"relu\")(ga)\n",
    "output = layers.Dense(1, activation = \"sigmoid\")(ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccc71487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 17)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 17, 128)           1373184   \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1373313 (5.24 MB)\n",
      "Trainable params: 1373313 (5.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs = input, outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0465f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "220/220 [==============================] - 7s 29ms/step - loss: 0.5227 - accuracy: 0.8046 - val_loss: 0.3571 - val_accuracy: 0.8974\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 5s 24ms/step - loss: 0.2750 - accuracy: 0.9241 - val_loss: 0.2492 - val_accuracy: 0.9124\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 5s 24ms/step - loss: 0.1974 - accuracy: 0.9409 - val_loss: 0.2215 - val_accuracy: 0.9205\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 5s 24ms/step - loss: 0.1603 - accuracy: 0.9527 - val_loss: 0.2158 - val_accuracy: 0.9256\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.1362 - accuracy: 0.9602 - val_loss: 0.2117 - val_accuracy: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x214885bbdc0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = \"binary_crossentropy\",\n",
    "    optimizer = \"adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "# train_sentences, val_sentences, train_labels, val_labels\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 5, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0479e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2de50ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "  -----\n",
    "  y_true = true labels in the form of a 1D array\n",
    "  y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score for class 1\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred)\n",
    "  model_results = {\"Accuracy\": [np.round(model_accuracy,2)],\n",
    "                  \"Precision\": [np.round(model_precision[1]* 100,2)],\n",
    "                  \"Recall\": [np.round(model_recall[1]* 100,2)],\n",
    "                  \"F1_score\": [np.round(model_f1[1]* 100,2)]}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e62e35eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': [92.65],\n",
       " 'Precision': [91.47],\n",
       " 'Recall': [89.89],\n",
       " 'F1_score': [90.67]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "model_results = calculate_results(y_true=y_valid,\n",
    "                                     y_pred=tf.round(y_pred_ebd))\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7622d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "        \"Description\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"F1_score\":[],\n",
    "        \"Accuracy\":[],\n",
    "        \"Pred_time_in_seconds\" : []\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a15366cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def result_log(model,text,X_test, y_true,result_df):\n",
    "  start_time = time.time()\n",
    "  y_pred = model.predict(X_test)\n",
    "  end_time = time.time()\n",
    "  pred_time = end_time - start_time\n",
    "  result_dict = calculate_results(y_true=y_valid,y_pred=tf.round(y_pred))\n",
    "  result_dict[\"Description\"] = text\n",
    "  result_dict[\"Pred_time_in_seconds\"] = [pred_time]\n",
    "  temp_df = pd.DataFrame(data = result_dict)\n",
    "  result_df = pd.concat([result_df, temp_df]).reset_index(drop = True)\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "380f1405",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_nb \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpalash\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfood_review_nb.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model_nb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model_nb = joblib.load(r\"C:\\Users\\palash\\Downloads\\food_review_nb.joblib\")\n",
    "model_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7af7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a085878",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_nb \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpalash\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfood_review_nb.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model_nb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model_nb = joblib.load(r\"C:\\Users\\palash\\Downloads\\food_review_nb.joblib\")\n",
    "model_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2205335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Pred_time_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Embedding model</td>\n",
       "      <td>91.47</td>\n",
       "      <td>89.89</td>\n",
       "      <td>90.67</td>\n",
       "      <td>92.65</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Description  Precision  Recall  F1_score  Accuracy  \\\n",
       "0  Embedding model      91.47   89.89     90.67     92.65   \n",
       "\n",
       "   Pred_time_in_seconds  \n",
       "0              0.214193  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_log(model_ebd,\"Embedding model\",X_valid, y_valid, result_df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34bbf885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01325517, -0.02686268, -0.02637479, ...,  0.00028612,\n",
       "         0.00683883, -0.05066485],\n",
       "       [-0.04962243,  0.03594908, -0.02034652, ...,  0.01815004,\n",
       "         0.03257399, -0.04494863],\n",
       "       [-0.04467124,  0.07065123,  0.03170334, ...,  0.00147509,\n",
       "         0.01003034, -0.07288383],\n",
       "       ...,\n",
       "       [ 0.0547647 , -0.04700103, -0.02073087, ..., -0.00491996,\n",
       "         0.07418086,  0.07791632],\n",
       "       [-0.08009841,  0.07165869,  0.05388992, ...,  0.07147376,\n",
       "        -0.07955992, -0.03975553],\n",
       "       [-0.02444868,  0.10294838,  0.09063026, ...,  0.09739957,\n",
       "        -0.07856586, -0.0991958 ]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ebd.get_layer(\"embedding_2\").get_weights()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a4593051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10728, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_ebd.get_layer(\"embedding_2\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f59903d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "  if num == 0:\n",
    "     continue # skip padding token\n",
    "  vec = embed_weights[num]\n",
    "  out_m.write(word + \"\\n\") # write words to file\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Download files locally to upload to Embedding Projector\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download(\"embedding_vectors.tsv\")\n",
    "  files.download(\"embedding_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d549ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 17)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 17, 128)           1373184   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 128)           0         \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 17, 4)             532       \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 4)                 36        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1373757 (5.24 MB)\n",
      "Trainable params: 1373757 (5.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "max_vocab_length = len(text_vectorizer.get_vocabulary())\n",
    "max_length = 17\n",
    "\n",
    "model_1_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "# Create rnn model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_1_embedding(x)\n",
    "# print(x.shape)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.SimpleRNN(4, return_sequences=True, dropout = 0.2, kernel_regularizer='l1')(x) # return vector for whole sequence\n",
    "# x = layers.SimpleRNN(8, return_sequences=True, dropout = 0.4, kernel_regularizer='l1')(x)\n",
    "x = layers.SimpleRNN(4, dropout = 0.2, kernel_regularizer='l1')(x)\n",
    "# print(x.shape)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_RNN\")\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3200ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "220/220 [==============================] - 10s 33ms/step - loss: 1.1302 - accuracy: 0.5556 - val_loss: 0.9600 - val_accuracy: 0.6026\n",
      "Epoch 2/6\n",
      "220/220 [==============================] - 7s 31ms/step - loss: 0.8381 - accuracy: 0.6358 - val_loss: 0.7457 - val_accuracy: 0.6739\n",
      "Epoch 3/6\n",
      "220/220 [==============================] - 7s 30ms/step - loss: 0.6617 - accuracy: 0.7318 - val_loss: 0.6627 - val_accuracy: 0.7197\n",
      "Epoch 4/6\n",
      "220/220 [==============================] - 7s 31ms/step - loss: 0.5665 - accuracy: 0.7958 - val_loss: 0.6482 - val_accuracy: 0.7248\n",
      "Epoch 5/6\n",
      "220/220 [==============================] - 7s 31ms/step - loss: 0.5035 - accuracy: 0.8319 - val_loss: 0.6535 - val_accuracy: 0.7363\n",
      "Epoch 6/6\n",
      "220/220 [==============================] - 7s 30ms/step - loss: 0.4574 - accuracy: 0.8515 - val_loss: 0.6647 - val_accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0005),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit model\n",
    "model_1_history = model_1.fit(X_train,\n",
    "                              y_train,\n",
    "                              epochs=6,\n",
    "                              validation_data=(X_valid, y_valid)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06ab91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Pred_time_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Embedding model</td>\n",
       "      <td>91.47</td>\n",
       "      <td>89.89</td>\n",
       "      <td>90.67</td>\n",
       "      <td>92.65</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn model</td>\n",
       "      <td>71.13</td>\n",
       "      <td>55.38</td>\n",
       "      <td>62.27</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.528572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Description  Precision  Recall  F1_score  Accuracy  \\\n",
       "0  Embedding model      91.47   89.89     90.67     92.65   \n",
       "1        rnn model      71.13   55.38     62.27     73.33   \n",
       "\n",
       "   Pred_time_in_seconds  \n",
       "0              0.214193  \n",
       "1              0.528572  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_log(model_1,\"rnn model\",X_valid, y_valid, result_df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a424cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 17, 128)\n",
      "(None, 8)\n",
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 17)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 17, 128)           1373184   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 17, 8)             4384      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1378121 (5.26 MB)\n",
      "Trainable params: 1378121 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "x = layers.LSTM(8, return_sequences=True, dropout = 0.4, kernel_regularizer='l1')(x) # return vector for whole sequence\n",
    "x = layers.LSTM(8)(x)\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7237fa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "220/220 [==============================] - 11s 35ms/step - loss: 1.6265 - accuracy: 0.8219 - val_loss: 0.3311 - val_accuracy: 0.9064\n",
      "Epoch 2/2\n",
      "220/220 [==============================] - 7s 31ms/step - loss: 0.2677 - accuracy: 0.9272 - val_loss: 0.2771 - val_accuracy: 0.9060\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit model\n",
    "model_2_history = model_2.fit(X_train,\n",
    "                              y_train,\n",
    "                              epochs=2,\n",
    "                              validation_data=(X_valid, y_valid)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2157499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Pred_time_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Embedding model</td>\n",
       "      <td>91.47</td>\n",
       "      <td>89.89</td>\n",
       "      <td>90.67</td>\n",
       "      <td>92.65</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn model</td>\n",
       "      <td>71.13</td>\n",
       "      <td>55.38</td>\n",
       "      <td>62.27</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.528572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lstm model</td>\n",
       "      <td>89.89</td>\n",
       "      <td>86.02</td>\n",
       "      <td>87.91</td>\n",
       "      <td>90.60</td>\n",
       "      <td>0.970244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Description  Precision  Recall  F1_score  Accuracy  \\\n",
       "0  Embedding model      91.47   89.89     90.67     92.65   \n",
       "1        rnn model      71.13   55.38     62.27     73.33   \n",
       "2       lstm model      89.89   86.02     87.91     90.60   \n",
       "\n",
       "   Pred_time_in_seconds  \n",
       "0              0.214193  \n",
       "1              0.528572  \n",
       "2              0.970244  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_log(model_2,\"lstm model\",X_valid, y_valid, result_df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e87fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0935132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\palash\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06244392 -0.05404239 -0.01550858 ... -0.01991867  0.03076827\n",
      "   0.00064757]\n",
      " [-0.03539826 -0.09636232 -0.00950034 ... -0.02322183  0.01694256\n",
      "  -0.0294972 ]], shape=(2, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([\"When you call the universal sentence encoder on a sentence\" ,\"it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61133b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c17e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model\n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25ff338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model using the Sequential API\n",
    "\n",
    "import tensorflow as tf\n",
    "model_3 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_3_USE\")\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(0.0008),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6901bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "220/220 [==============================] - 5s 13ms/step - loss: 0.3386 - accuracy: 0.8725 - val_loss: 0.2164 - val_accuracy: 0.9231\n",
      "Epoch 2/4\n",
      "220/220 [==============================] - 3s 12ms/step - loss: 0.1951 - accuracy: 0.9337 - val_loss: 0.2001 - val_accuracy: 0.9231\n",
      "Epoch 3/4\n",
      "220/220 [==============================] - 2s 11ms/step - loss: 0.1829 - accuracy: 0.9373 - val_loss: 0.1960 - val_accuracy: 0.9226\n",
      "Epoch 4/4\n",
      "220/220 [==============================] - 2s 10ms/step - loss: 0.1763 - accuracy: 0.9386 - val_loss: 0.1946 - val_accuracy: 0.9248\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of pretrained embeddings\n",
    "\n",
    "model_3_history = model_3.fit(X_train,\n",
    "                              y_train,\n",
    "                              epochs=4,\n",
    "                              validation_data=(X_valid, y_valid)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eeae8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Pred_time_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Embedding model</td>\n",
       "      <td>91.47</td>\n",
       "      <td>89.89</td>\n",
       "      <td>90.67</td>\n",
       "      <td>92.65</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn model</td>\n",
       "      <td>71.13</td>\n",
       "      <td>55.38</td>\n",
       "      <td>62.27</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.528572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lstm model</td>\n",
       "      <td>89.89</td>\n",
       "      <td>86.02</td>\n",
       "      <td>87.91</td>\n",
       "      <td>90.60</td>\n",
       "      <td>0.970244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use model</td>\n",
       "      <td>90.28</td>\n",
       "      <td>90.86</td>\n",
       "      <td>90.57</td>\n",
       "      <td>92.48</td>\n",
       "      <td>0.977688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Description  Precision  Recall  F1_score  Accuracy  \\\n",
       "0  Embedding model      91.47   89.89     90.67     92.65   \n",
       "1        rnn model      71.13   55.38     62.27     73.33   \n",
       "2       lstm model      89.89   86.02     87.91     90.60   \n",
       "3        use model      90.28   90.86     90.57     92.48   \n",
       "\n",
       "   Pred_time_in_seconds  \n",
       "0              0.214193  \n",
       "1              0.528572  \n",
       "2              0.970244  \n",
       "3              0.977688  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_log(model_3,\"use model\",X_valid, y_valid, result_df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "58c6b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Downloads\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Downloads\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model_3, \"Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "441d3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text, model):\n",
    "  text = text_cleaner(text)\n",
    "  y_pred = model.predict([text])\n",
    "  print((\"Negative Review\" if y_pred[0] == 1 else \"Positive Review\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d21e99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palash\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_nb \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpalash\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfood_review_nb.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model_nb = joblib.load(r\"C:\\Users\\palash\\Downloads\\food_review_nb.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc5204f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"atmosphere is ok but food is not good\"\n",
    "text2 = \"atmosphere is ok but food is bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ffb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
